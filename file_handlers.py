import json
import logging
import os
from copy import deepcopy
from datetime import datetime
from json import JSONDecodeError
from typing import List, Optional, Tuple
from urllib.parse import urlparse

import requests
from bs4 import BeautifulSoup

from custom_markdown_convert import html2md

logger = logging.getLogger(__name__)

# å®šä¹‰æ—¥å¿—æ–‡ä»¶è·¯å¾„
log_file_path = 'file_handlers_log.log'

# é…ç½®æ—¥å¿—è®°å½•å™¨
logging.basicConfig(
    level=logging.ERROR,  # è®¾ç½®æ—¥å¿—çº§åˆ«ï¼Œè¿™é‡Œæ˜¯DEBUG
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',  # æ—¥å¿—æ ¼å¼
    handlers=[
        logging.FileHandler(log_file_path),  # å°†æ—¥å¿—è¾“å‡ºåˆ°æ–‡ä»¶
        logging.StreamHandler()  # åŒæ—¶ä¿ç•™æ§åˆ¶å°è¾“å‡º
    ]
)
session = requests.Session()
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}


def fetch_page(url: str) -> Optional[str]:
    """è·å–é¡µé¢å†…å®¹"""
    try:
        logger.debug(f'æ­£åœ¨çˆ¬å–: {url}')
        response = session.get(url, headers=headers, timeout=2)
        response.encoding = response.apparent_encoding
        if 'text/html' in response.headers.get('Content-Type', ''):
            return response.text
        else:
            logger.info(f'âŒ å†…å®¹ä¸æ˜¯ text/html: {url}')
            return None
    except (requests.exceptions.RequestException, requests.exceptions.Timeout) as e:
        logger.error(f'âŒ è¯·æ±‚é”™è¯¯: {url}: {e}')
        return None


def extract_content(soup: BeautifulSoup, target_area_content_tags: List[str]) -> str:
    """
    æå–BeautifulSoupå¯¹è±¡ä¸­å¤šä¸ªç‰¹å®šæ ‡ç­¾çš„å†…å®¹ï¼Œå¹¶æ‹¼æ¥æˆä¸€ä¸ªæ–°çš„HTMLå­—ç¬¦ä¸²

    :param soup: BeautifulSoup, åŸå§‹çš„BeautifulSoupå¯¹è±¡
    :param target_tags: List[str], ç›®æ ‡æ ‡ç­¾åç§°åˆ—è¡¨
    :return: str, åŒ…å«æå–å†…å®¹çš„æ–°çš„HTMLå­—ç¬¦ä¸²
    """
    if not target_area_content_tags:
        return str(soup)
    new_soup = BeautifulSoup('<html><head><meta charset="utf-8"></head><body></body></html>', 'html.parser')
    soup_copy = deepcopy(soup)
    extracted_tags = set()
    for tag_name in target_area_content_tags:
        if tag_name not in extracted_tags:
            tags = soup_copy.find_all(tag_name)
            for tag in tags:
                new_soup.body.append(tag)
            extracted_tags.add(tag_name)
    return str(new_soup)


def extract_common_file_urls(links: List[Tuple[str, str]]) -> List[Tuple[str, str]]:
    """æå–å¸¸è§æ–‡ä»¶é“¾æ¥"""
    common_file_extensions = [
        '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.zip', '.rar', '.7z',
        '.txt', '.rtf', '.md', '.xml', '.csv', '.json', '.log', '.ini', '.cfg', '.yaml', '.yml'
    ]
    return [(link, title) for link, title in links if any(link.lower().endswith(ext) for ext in common_file_extensions)]


def record_page_info(url: str, file_path: str, file_links: dict, output_json_file: str) -> None:
    date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    """è®°å½•é¡µé¢ä¿¡æ¯åˆ°JSONæ–‡ä»¶ï¼Œé¿å…é‡å¤URLï¼Œå¦‚å­˜åœ¨åˆ™æ›´æ–°ä¿¡æ¯"""

    page_info = {
        'url': url,
        'file_path': file_path,
        'file_links': file_links,
        "date": date,
    }

    if os.path.exists(output_json_file):
        try:
            with open(output_json_file, 'r') as json_file:
                existing_data = json.load(json_file)
        except JSONDecodeError:
            existing_data = []

        updated = False
        for index, entry in enumerate(existing_data):
            if entry['url'] == url:
                existing_data[index].update(page_info)
                updated = True
                break
        if not updated:
            existing_data.append(page_info)

        with open(output_json_file, 'w') as json_file:
            json.dump(existing_data, json_file, indent=2, ensure_ascii=False)
    else:
        with open(output_json_file, 'w') as json_file:
            json.dump([page_info], json_file, indent=2, ensure_ascii=False)


# å…¨å±€å˜é‡å­˜å‚¨æ–‡ä»¶ç±»å‹
FILE_TYPES = {}


def load_file_types():
    """åŠ è½½æ–‡ä»¶ç±»å‹"""
    global FILE_TYPES
    if FILE_TYPES:
        return
    with open('file_types.json', 'r') as file:
        FILE_TYPES = json.load(file)


def download_files(file_urls: List[Tuple[str, str]], download_dir: str, already_downloaded: set) -> None:
    """ä¸‹è½½æ–‡ä»¶å¹¶åˆ†ç±»"""
    load_file_types()
    dir_path = os.path.join(os.path.dirname(__file__), download_dir)
    os.makedirs(dir_path, exist_ok=True)

    for file_url, file_name in file_urls:
        if file_url not in already_downloaded:
            # ä» URL ä¸­æå–æ–‡ä»¶åç¼€
            file_ext = file_url.split('.')[-1].split('?')[0].lower()
            file_category = FILE_TYPES.get(file_ext, 'å…¶ä»–æ–‡ä»¶')
            file_path = os.path.join(download_dir, file_category, f"{file_name}.{file_ext}")
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            try:
                response = session.get(file_url, headers=headers)
                response.raise_for_status()
                with open(file_path, 'wb') as file:
                    file.write(response.content)
                logger.info(f'ğŸ“¥ ä¸‹è½½æ–‡ä»¶: {file_name}.{file_ext}')
            except requests.RequestException as e:
                logger.error(f'âŒ æ–‡ä»¶ä¸‹è½½å¤±è´¥: {file_name}.{file_ext} - {e}')


def extract_url_title_name(url: str, soup: BeautifulSoup) -> str:
    """
    ä¼˜åŒ–åœ°ä»URLå’Œé¡µé¢æ ‡é¢˜ä¸­æå–æ›´è§„èŒƒçš„æ–‡ä»¶åã€‚

    1. ä½¿ç”¨URLçš„è·¯å¾„éƒ¨åˆ†æ¥æ¨æµ‹æ–‡ä»¶åï¼Œå¦‚æœå­˜åœ¨ã€‚
    2. å¦‚æœæ²¡æœ‰è·¯å¾„æˆ–è·¯å¾„ä¸åŒ…å«æœ‰æ„ä¹‰çš„æ–‡ä»¶åä¿¡æ¯ï¼Œåˆ™ä½¿ç”¨é¡µé¢æ ‡é¢˜ã€‚
    3. æ¸…ç†æ ‡é¢˜ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦å¹¶è½¬æ¢ä¸ºç©ºæ ¼ï¼Œç„¶åæ›¿æ¢ç©ºæ ¼ä¸ºçŸ­æ¨ªçº¿ã€‚
    4. ç¡®ä¿æ–‡ä»¶åä¸ä»¥çŸ­æ¨ªçº¿å¼€å¤´æˆ–ç»“å°¾ã€‚
    """

    # è·å–é¡µé¢æ ‡é¢˜å¹¶æ¸…ç†
    title = soup.title.string.strip("/") if soup.title else "Untitled"

    parsed_url = urlparse(url)
    path = parsed_url.path

    if path:

        filename_from_path = path.replace("/", "_")
        file_name = f"{title}-{filename_from_path}"
    else:
        file_name = title

    file_name = file_name.strip('/')

    return file_name


def save_content(file_path: str, content: str, filter_tags: List[str]) -> None:
    """ä¿å­˜å†…å®¹åˆ°Markdownæ–‡ä»¶"""
    if content:
        strip_elements = ['img']
        strip_elements.extend(filter_tags)
        output = html2md(content, strip=strip_elements)
        logger.info(f'åˆ›å»º ğŸ“ {file_path.split("/")[-1]}')

        with open(file_path, 'w') as f:
            f.write(output)
    else:
        logger.error(f'âŒ ç©ºå†…å®¹: {file_path}. è¯·æ£€æŸ¥ç›®æ ‡å…ƒç´ ï¼Œè·³è¿‡ã€‚')
